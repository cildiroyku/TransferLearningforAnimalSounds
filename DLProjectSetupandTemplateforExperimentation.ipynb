{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMI0TLF0AVtsvZ5H+wHoo9t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cildiroyku/TransferLearningforAnimalSounds/blob/main/DLProjectSetupandTemplateforExperimentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning Notebook\n",
        "\n",
        "Instructions:\n",
        "\n",
        "* This notebook has dependencies from a GitHub repo for the project that includes a config file, the path to YAMNet, embedding transofrmations for PyTorch. **Run Part 1 completely before starting to work on this notebook.**\n",
        "\n",
        "* Part 2 is for demonstration purposes only on one sample audio clip. It was used to investigate the inner workings of YAMNet and understand how the embeddings need to be extracted and structured, to be able to process them. **Part 2 is optional to run.**\n",
        "\n",
        "* Part 3 sets up a pipeline to extract embeddings. **Part 3 it should be run before proceeding.**\n",
        "\n",
        "*"
      ],
      "metadata": {
        "id": "Zbux8G-IjCdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part1: Setting up to clone the repo from GitHub, get dependencies, model weights and connect to GPU\n",
        "\n",
        "No need to modify this section. If GPU is not available, model will run on CPU.\n"
      ],
      "metadata": {
        "id": "BoP9RMgvQkkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cildiroyku/TransferLearningforAnimalSounds.git\n",
        "%cd TransferLearningforAnimalSounds\n",
        "!ls -lh yamnet.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t6QCzRuJM98",
        "outputId": "8d251662-65ad-4cea-bfb7-84b8bf178dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TransferLearningforAnimalSounds'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 32 (delta 6), reused 12 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (32/32), 13.34 MiB | 13.23 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "/content/TransferLearningforAnimalSounds\n",
            "-rw-r--r-- 1 root root 15M Nov 11 16:04 yamnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports and configuration load"
      ],
      "metadata": {
        "id": "x0-bZsbpOvJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "cfg = yaml.safe_load(open(\"configs/base.yaml\"))\n",
        "print(cfg[\"data\"][\"sample_rate\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvExCLz_LXec",
        "outputId": "89b501f4-5d05-41d3-9e29-d0891e7bd6c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_config.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKnY20oqOY0W",
        "outputId": "423369d9-e054-44ba-ce2f-fdb3ba65ea93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config loaded successfully!\n",
            "Project name: transfer_learning_for_animal_sounds\n",
            "Sample rate: 16000\n",
            "Device preference: cuda\n",
            "\n",
            "Verified folder: data/gtzan/\n",
            "Verified folder: data/esc50/\n",
            "Verified folder: outputs/embeddings/\n",
            "\n",
            "GPU not detected ‚Äî using CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/w-hc/torch_audioset.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1J5cZOKSnVh",
        "outputId": "32841979-4fc1-4990-b028-cc47c0c99e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torch_audioset'...\n",
            "remote: Enumerating objects: 269, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 269 (delta 28), reused 26 (delta 26), pack-reused 237 (from 1)\u001b[K\n",
            "Receiving objects: 100% (269/269), 662.51 KiB | 8.83 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls torch_audioset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymD-LJePTHMx",
        "outputId": "d16a4140-b0d1-49aa-e54b-c1d3a6c20ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LICENSE    README.md  test\t  tools\t\t  visualization.py\n",
            "notebooks  setup.py   tf_2_torch  torch_audioset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find torch_audioset -maxdepth 2 -type f -name \"*yamnet*.py\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI9s2Fj7TONz",
        "outputId": "f225a45b-8b39-4bb6-8255-232562725297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch_audioset/tf_2_torch/store_yamnet_pred_metadata_as_yml.py\n",
            "torch_audioset/tf_2_torch/convert_yamnet.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!realpath torch_audioset/tf_2_torch/convert_yamnet.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7krepf8T8cu",
        "outputId": "45115f15-140e-4ca4-c417-e3453434c68e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TransferLearningforAnimalSounds/torch_audioset/tf_2_torch/convert_yamnet.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find torch_audioset -type f -name \"params.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSc8fiYMUpRC",
        "outputId": "4c223563-b12c-4818-949b-17f52ec411c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch_audioset/torch_audioset/params.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find torch_audioset -type f -name \"model.py\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9sF8BmKVeFq",
        "outputId": "1d8f8ab1-f0c6-463c-be84-b2aedd59d275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch_audioset/torch_audioset/yamnet/model.py\n",
            "torch_audioset/torch_audioset/vggish/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/TransferLearningforAnimalSounds/torch_audioset\")\n",
        "\n",
        "from torch_audioset.yamnet.model import yamnet as YamNet"
      ],
      "metadata": {
        "id": "rpv0iFKVWYhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/TransferLearningforAnimalSounds\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JjXJ26gWrwq",
        "outputId": "090bcb20-335e-4857-889a-38e63336e3f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TransferLearningforAnimalSounds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls configs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN4AOQ1VW38t",
        "outputId": "04cfc9ad-0061-42fb-d7bd-3514efa6580f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, yaml\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "with open(\"configs/base.yaml\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "model = YamNet().to(device)\n",
        "checkpoint = torch.load(\"yamnet.pth\", map_location=device)\n",
        "model.load_state_dict(checkpoint)\n",
        "model.eval()\n",
        "\n",
        "print(\"YAMNet model ready on\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkhe5BTUWbX4",
        "outputId": "1d450033-c25f-4cfe-c989-d424880ff522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/w-hc/torch_audioset/releases/download/v0.1/yamnet.pth\" to /root/.cache/torch/hub/checkpoints/yamnet.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.4M/14.4M [00:00<00:00, 126MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YAMNet model ready on cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2:Testing YAMNet on a sample, real audio clip from the ESC-50 dataset\n",
        "\n",
        "**What this section does**\n",
        "\n",
        "* End-to-end flow from raw audio ‚Üí embedding\n",
        "* Fixing TensorFlow--> Pytorch compatibility for embedding sizes\n",
        "* Creating a custom function to extract yamnet embeddings up to layer14\n",
        "* Embedding matching of the correct feature dimension"
      ],
      "metadata": {
        "id": "N0F3uWAYX5X_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading a test sound to check for sample rate and waveform"
      ],
      "metadata": {
        "id": "WTVM6JkvXNob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import numpy as np\n",
        "\n",
        "# Example environmental sound (dog bark)\n",
        "url = \"https://github.com/karoldvl/ESC-50/raw/master/audio/1-100032-A-0.wav\"\n",
        "waveform, sr = torchaudio.load(url)\n",
        "\n",
        "print(\"Original sample rate:\", sr, \"| waveform shape:\", waveform.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1MGu3uKXMq7",
        "outputId": "0aa767fa-dfaa-441b-eb62-476c23c96be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sample rate: 44100 | waveform shape: torch.Size([1, 220500])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing audio file imported aboce"
      ],
      "metadata": {
        "id": "MEcSlEWVXdsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_sr = cfg[\"data\"][\"sample_rate\"]\n",
        "\n",
        "# Resample if needed\n",
        "if sr != target_sr:\n",
        "    waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
        "\n",
        "# Convert to mono and normalize\n",
        "waveform = waveform.mean(dim=0, keepdim=True)\n",
        "waveform = waveform / waveform.abs().max()\n",
        "\n",
        "print(\"Processed waveform shape:\", waveform.shape)  # [1, N]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_sFgCQIYYVM",
        "outputId": "0704fbd4-604c-4e49-e13b-9f6f1713cef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed waveform shape: torch.Size([1, 80000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters consistent with YAMNet's frontend\n",
        "mel_transform = T.MelSpectrogram(\n",
        "    sample_rate=target_sr,\n",
        "    n_fft=1024,\n",
        "    hop_length=160,\n",
        "    n_mels=64,\n",
        "    f_min=125,\n",
        "    f_max=7500,\n",
        ")\n",
        "db_transform = T.AmplitudeToDB(stype=\"power\")\n",
        "\n",
        "# Convert\n",
        "mel_spec = mel_transform(waveform)\n",
        "mel_spec_db = db_transform(mel_spec)\n",
        "\n",
        "print(\"Mel-spectrogram shape:\", mel_spec_db.shape)  # [1, 64, T]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltnnN5UKXafw",
        "outputId": "cdbe810b-abeb-404c-c54a-8e1f4ff3afbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mel-spectrogram shape: torch.Size([1, 64, 501])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YAMNet expects [frames, mel_bins], not batched [1, 64, T]\n",
        "mel_spec_db = mel_spec_db.transpose(1, 2)  # ‚Üí [1, T, 64]\n",
        "mel_spec_db = mel_spec_db.squeeze(0)       # ‚Üí [T, 64]\n",
        "\n",
        "print(\"Final mel input shape for YAMNet:\", mel_spec_db.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS6MwyHDa8MI",
        "outputId": "35b39112-a16d-48a1-d867-e8b2a06b82de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final mel input shape for YAMNet: torch.Size([501, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mel_spec_db = mel_spec_db.unsqueeze(0).unsqueeze(0)\n",
        "print(\"Final mel input shape for YAMNet:\", mel_spec_db.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZPkW-YtcEXg",
        "outputId": "01eeb9cf-14e3-4378-ed8a-630d029e503c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final mel input shape for YAMNet: torch.Size([1, 1, 501, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Jq7ApvJsdCXm",
        "outputId": "86baae4d-6d3b-4924-ed56-7f363e0a15bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YAMNet(\n",
            "  (layer1): Conv(\n",
            "    (fused): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer2): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=32, bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer3): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=SAME, groups=64, bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(64, 128, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer4): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=128, bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer5): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=SAME, groups=128, bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(128, 256, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer6): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=256, bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(256, 256, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer7): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=SAME, groups=256, bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(256, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer8): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=512, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer9): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=512, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer10): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=512, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer11): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=512, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer12): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=512, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer13): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=SAME, groups=512, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 1024, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(1024, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer14): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=1024, bias=False)\n",
            "      (bn): BatchNorm2d(1024, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(1024, 1024, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(1024, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=1024, out_features=521, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_yamnet_embedding(model, mel_features):\n",
        "    \"\"\"\n",
        "    Runs YAMNet backbone up to layer14 ‚Üí returns 1024-D embedding\n",
        "    Input shape: [1, 1, frames, 64]\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        x = model.layer1(mel_features)\n",
        "        x = model.layer2(x)\n",
        "        x = model.layer3(x)\n",
        "        x = model.layer4(x)\n",
        "        x = model.layer5(x)\n",
        "        x = model.layer6(x)\n",
        "        x = model.layer7(x)\n",
        "        x = model.layer8(x)\n",
        "        x = model.layer9(x)\n",
        "        x = model.layer10(x)\n",
        "        x = model.layer11(x)\n",
        "        x = model.layer12(x)\n",
        "        x = model.layer13(x)\n",
        "        x = model.layer14(x)\n",
        "\n",
        "        # Global average pooling across spatial dims\n",
        "        x = torch.mean(x, dim=[2, 3])  # [batch, 1024]\n",
        "        return x.squeeze(0)\n"
      ],
      "metadata": {
        "id": "5u_Dang6ZN0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passing through YAMNet to extract embeddings"
      ],
      "metadata": {
        "id": "KfBR5EdXXnTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_embedding = extract_yamnet_embedding(model, mel_spec_db)\n",
        "print(\"Embedding shape:\", clip_embedding.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-0BjH1Cdb9V",
        "outputId": "c7e2376b-5f33-4bd3-f0dd-cceb2873bde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: torch.Size([1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Making a reusable pipeline to extract YAMNet embeddings"
      ],
      "metadata": {
        "id": "RDvlkcQ1eLay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torch\n",
        "\n",
        "def yamnet_extract_from_file(model, file_path, cfg):\n",
        "    \"\"\"\n",
        "    Full pipeline: loads audio, converts to log-mel, extracts 1024-D embedding.\n",
        "    Returns a PyTorch tensor [1024].\n",
        "    \"\"\"\n",
        "    target_sr = cfg[\"data\"][\"sample_rate\"]\n",
        "\n",
        "    # --- Load and resample ---\n",
        "    waveform, sr = torchaudio.load(file_path)\n",
        "    if sr != target_sr:\n",
        "        waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
        "    waveform = waveform.mean(dim=0, keepdim=True)\n",
        "    waveform = waveform / waveform.abs().max()\n",
        "\n",
        "    # --- Mel spectrogram ---\n",
        "    mel_transform = T.MelSpectrogram(\n",
        "        sample_rate=target_sr,\n",
        "        n_fft=1024,\n",
        "        hop_length=160,\n",
        "        n_mels=64,\n",
        "        f_min=125,\n",
        "        f_max=7500,\n",
        "    )\n",
        "    db_transform = T.AmplitudeToDB(stype=\"power\")\n",
        "    mel_spec = mel_transform(waveform)\n",
        "    mel_spec_db = db_transform(mel_spec)\n",
        "\n",
        "    # --- Prepare for YAMNet ---\n",
        "    mel_spec_db = mel_spec_db.transpose(1, 2).unsqueeze(0)\n",
        "\n",
        "\n",
        "    # --- Forward through YAMNet backbone ---\n",
        "    with torch.no_grad():\n",
        "        x = model.layer1(mel_spec_db)\n",
        "        x = model.layer2(x)\n",
        "        x = model.layer3(x)\n",
        "        x = model.layer4(x)\n",
        "        x = model.layer5(x)\n",
        "        x = model.layer6(x)\n",
        "        x = model.layer7(x)\n",
        "        x = model.layer8(x)\n",
        "        x = model.layer9(x)\n",
        "        x = model.layer10(x)\n",
        "        x = model.layer11(x)\n",
        "        x = model.layer12(x)\n",
        "        x = model.layer13(x)\n",
        "        x = model.layer14(x)\n",
        "        x = torch.mean(x, dim=[2, 3])  # [batch, 1024]\n",
        "\n",
        "    return x.squeeze(0)"
      ],
      "metadata": {
        "id": "GaULiDpQeOve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/karoldvl/ESC-50/raw/master/audio/1-100032-A-0.wav\"\n",
        "waveform, sr = torchaudio.load(url)"
      ],
      "metadata": {
        "id": "XuZr5-z3iU74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample usage:\n",
        "\n",
        "path = \"/content/TransferLearningforAnimalSounds/data/esc50/1-100032-A-0.wav\"\n",
        "torch.save(waveform, \"sample_waveform.pt\")\n",
        "emb = yamnet_extract_from_file(model, url, cfg)\n",
        "\n",
        "print(emb.shape)  # torch.Size([1024])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5XpejrTeSZw",
        "outputId": "38ff92d3-a3b5-4718-98bd-280818c37c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Loading the Data for Extraction\n",
        "\n",
        "Make sure you point the extraction function below to where you have the audio files from ESC-50 saved on your Google Drive."
      ],
      "metadata": {
        "id": "Js_A7cNEkvAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK4p3z_Npx3R",
        "outputId": "983126dc-48e4-4743-b359-ac3a107733ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/DeepLearning/ESC-50-master/audio\" /content/audio"
      ],
      "metadata": {
        "id": "n1buJMzHqBUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Creating a Pipeline That Processes All Audio Clips in Batch\n",
        "\n",
        "Building a function that:\n",
        "\n",
        "* walks through a dataset folder (e.g. data/esc50),\n",
        "\n",
        "* extracts one 1024-D embedding per audio file,\n",
        "\n",
        "* saves each embedding to a structured directory (outputs/embeddings/{dataset_name}/),\n",
        "\n",
        "* writes a summary .csv or .npy file for downstream training."
      ],
      "metadata": {
        "id": "7QOXXw-becjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "def extract_embeddings_from_folder(model, folder_path, cfg, save_dir):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    embeddings, labels = [], []\n",
        "\n",
        "    # Glob reference folder_path\n",
        "    wav_files = glob.glob(os.path.join(folder_path, \"*.wav\"))\n",
        "    print(f\"üîç Found {len(wav_files)} WAV files in: {folder_path}\")\n",
        "\n",
        "    if len(wav_files) == 0:\n",
        "        print(\"‚ö†Ô∏è No .wav files found ‚Äî check your folder path!\")\n",
        "        return\n",
        "\n",
        "    for file_path in tqdm(wav_files):\n",
        "        emb = yamnet_extract_from_file(model, file_path, cfg)\n",
        "        embeddings.append(emb.numpy())\n",
        "        labels.append(os.path.basename(file_path).split(\"-\")[0])  # optional: parse label\n",
        "\n",
        "    embeddings = np.stack(embeddings)\n",
        "    np.save(os.path.join(save_dir, \"embeddings.npy\"), embeddings)\n",
        "    np.save(os.path.join(save_dir, \"labels.npy\"), np.array(labels))\n",
        "    print(f\"‚úÖ Saved embeddings: {embeddings.shape} | Labels: {len(labels)}\")"
      ],
      "metadata": {
        "id": "VWmHl0hwegUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_embeddings_from_folder(\n",
        "    model,\n",
        "    \"/content/drive/MyDrive/DeepLearning/ESC-50-master/audio\",\n",
        "    cfg,\n",
        "    \"outputs/embeddings/esc50\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4DS_6Qyk7Qg",
        "outputId": "111ad0e7-2672-4076-a5da-14d40e40fb84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Found 2000 WAV files in: /content/drive/MyDrive/DeepLearning/ESC-50-master/audio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [02:54<00:00, 11.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved embeddings: (2000, 1024) | Labels: 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GTZAN Segment Based Extraction\n",
        "\n",
        "GTZAN audio files are 30 sec long, which are way larger than the less than 10s expected length for YAMNet. So we proceed with segment-based embedding extraction.\n",
        "\n",
        "Handles genre subfolders automatically (blues, rock, etc.)\n",
        "\n",
        "Segments each 30s clip into overlapping 3s windows\n",
        "\n",
        "Converts to Mel spectrograms consistent with YAMNet frontend\n",
        "\n",
        "Performs embedding extraction manually (like your ESC-50 extractor backbone)\n",
        "\n",
        "Optionally aggregates per song (or saves every segment)\n",
        "\n",
        "Saves embeddings.npy + labels.npy in your chosen output folder"
      ],
      "metadata": {
        "id": "ZohxW-cOvlDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The extraction for GTZAN (it is different since the shapes are different)\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "def extract_embeddings_from_gtzan(model, folder_path, cfg, save_dir,\n",
        "                                  segment_length=3.0, overlap=0.5, aggregate=True):\n",
        "    \"\"\"\n",
        "    Extracts YAMNet embeddings from GTZAN dataset (30s music clips).\n",
        "    Includes segmentation and Mel spectrogram transform for each segment.\n",
        "\n",
        "    Args:\n",
        "        model: Loaded YAMNet model.\n",
        "        folder_path: Path to GTZAN root folder (contains genre subfolders).\n",
        "        cfg: Configuration dictionary (for sample rate etc.).\n",
        "        save_dir: Directory where embeddings and labels are saved.\n",
        "        segment_length: Segment duration in seconds (default = 3.0).\n",
        "        overlap: Overlap fraction between segments (default = 0.5).\n",
        "        aggregate: Whether to average embeddings per song (True) or save all segments (False).\n",
        "\n",
        "    Saves:\n",
        "        embeddings.npy (shape: [n_songs, 1024]) or [n_segments, 1024]\n",
        "        labels.npy (list of corresponding genre labels)\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    target_sr = cfg[\"data\"][\"sample_rate\"]\n",
        "\n",
        "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=target_sr,\n",
        "        n_fft=1024,\n",
        "        hop_length=160,\n",
        "        n_mels=64,\n",
        "        f_min=125,\n",
        "        f_max=7500,\n",
        "    )\n",
        "    db_transform = torchaudio.transforms.AmplitudeToDB(stype='power')\n",
        "\n",
        "    embeddings, labels = [], []\n",
        "\n",
        "    # loop through genre folders\n",
        "    for genre_folder in sorted(os.listdir(folder_path)):\n",
        "        genre_path = os.path.join(folder_path, genre_folder)\n",
        "        if not os.path.isdir(genre_path):\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nüéµ Processing genre: {genre_folder}\")\n",
        "        wav_files = glob.glob(os.path.join(genre_path, \"*.wav\"))\n",
        "\n",
        "        for i, file_path in enumerate(tqdm(wav_files, desc=genre_folder)):\n",
        "            try:\n",
        "                waveform, sr = torchaudio.load(file_path)\n",
        "                if sr != target_sr:\n",
        "                    waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
        "\n",
        "                # Convert stereo ‚Üí mono\n",
        "                waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "                # Segment audio\n",
        "                samples_per_seg = int(target_sr * segment_length)\n",
        "                hop_size = int(samples_per_seg * (1 - overlap))\n",
        "                segments = []\n",
        "                for start in range(0, waveform.shape[1] - samples_per_seg + 1, hop_size):\n",
        "                    segment = waveform[:, start:start + samples_per_seg]\n",
        "                    segments.append(segment)\n",
        "\n",
        "                song_embeds = []\n",
        "                for seg in segments:\n",
        "                    # Convert to mel-spectrogram\n",
        "                    mel_spec = mel_transform(seg)\n",
        "                    mel_spec_db = db_transform(mel_spec)\n",
        "                    mel_spec_db = mel_spec_db.transpose(1, 2).unsqueeze(0)  # [1, 1, frames, mel_bins]\n",
        "  # [1, 1, frames, mel_bins]\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        x = model.layer1(mel_spec_db)\n",
        "                        x = model.layer2(x)\n",
        "                        x = model.layer3(x)\n",
        "                        x = model.layer4(x)\n",
        "                        x = model.layer5(x)\n",
        "                        x = model.layer6(x)\n",
        "                        x = model.layer7(x)\n",
        "                        x = model.layer8(x)\n",
        "                        x = model.layer9(x)\n",
        "                        x = model.layer10(x)\n",
        "                        x = model.layer11(x)\n",
        "                        x = model.layer12(x)\n",
        "                        x = model.layer13(x)\n",
        "                        x = model.layer14(x)\n",
        "                        x = torch.mean(x, dim=[2, 3])  # global average pooling\n",
        "                        song_embeds.append(x.squeeze(0).cpu().numpy())\n",
        "\n",
        "                # Aggregate segments per song\n",
        "                if len(song_embeds) > 0:\n",
        "                    if aggregate:\n",
        "                        emb = np.mean(song_embeds, axis=0)\n",
        "                        embeddings.append(emb)\n",
        "                        labels.append(genre_folder)\n",
        "                    else:\n",
        "                        for emb in song_embeds:\n",
        "                            embeddings.append(emb)\n",
        "                            labels.append(genre_folder)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {file_path}: {e}\")\n",
        "\n",
        "    if len(embeddings) == 0:\n",
        "        print(\"‚ö†Ô∏è No embeddings extracted ‚Äî check your folder or file paths!\")\n",
        "        return\n",
        "\n",
        "    embeddings = np.stack(embeddings)\n",
        "    np.save(os.path.join(save_dir, \"embeddings.npy\"), embeddings)\n",
        "    np.save(os.path.join(save_dir, \"labels.npy\"), np.array(labels))\n",
        "    print(f\"\\n‚úÖ Saved embeddings: {embeddings.shape} | Labels: {len(labels)}\")\n"
      ],
      "metadata": {
        "id": "kAfsKG36LKud"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_embeddings_from_gtzan(\n",
        "    model,\n",
        "    \"/content/drive/MyDrive/DeepLearning/GTZAN/genres_original\",\n",
        "    cfg,\n",
        "    \"outputs/embeddings/gtzan\",\n",
        "    segment_length=3.0,  # seconds\n",
        "    overlap=0.5,         # 50% overlap\n",
        "    aggregate=True        # average across segments per song\n",
        ")"
      ],
      "metadata": {
        "id": "ahlmr8CptDQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8b491eb5-22c9-417a-841e-8d87f6436661"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéµ Processing genre: blues\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "blues: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:53<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéµ Processing genre: classical\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "classical: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:49<00:00,  2.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéµ Processing genre: country\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "country: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:49<00:00,  2.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéµ Processing genre: disco\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "disco: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:48<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéµ Processing genre: hiphop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "hiphop: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:48<00:00,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéµ Processing genre: jazz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "jazz:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/100 [00:21<00:24,  2.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping /content/drive/MyDrive/DeepLearning/GTZAN/genres_original/jazz/jazz.00054.wav: Failed to open the input \"/content/drive/MyDrive/DeepLearning/GTZAN/genres_original/jazz/jazz.00054.wav\" (Invalid data found when processing input).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "jazz: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:47<00:00,  2.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéµ Processing genre: metal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "metal: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:47<00:00,  2.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéµ Processing genre: pop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "pop: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:49<00:00,  2.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéµ Processing genre: reggae\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reggae: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:48<00:00,  2.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéµ Processing genre: rock\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rock: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:49<00:00,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Saved embeddings: (999, 1024) | Labels: 999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Loading embeddings as predictors and labels\n",
        "\n"
      ],
      "metadata": {
        "id": "gF60jDgvse5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X_esc = np.load(\"outputs/embeddings/esc50/embeddings.npy\")\n",
        "y_esc = np.load(\"outputs/embeddings/esc50/labels.npy\")\n",
        "\n",
        "X_gtzan = np.load(\"outputs/embeddings/gtzan/embeddings.npy\")\n",
        "y_gtzan = np.load(\"outputs/embeddings/gtzan/labels.npy\")\n",
        "\n",
        "print(X_esc.shape, y_esc.shape)\n",
        "print(X_gtzan.shape, y_gtzan.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlP2QU6Os_cv",
        "outputId": "7d1e5df5-156a-4787-a13c-83921747f1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2000, 1024) (2000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-Shot\n",
        "Train: GTZAN\n",
        "Test: ESC-50"
      ],
      "metadata": {
        "id": "Fc0R1qjHsWUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = X_gtzan, y_gtzan\n",
        "X_test,  y_test  = X_esc,  y_esc"
      ],
      "metadata": {
        "id": "0E05RlHGvTiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7: Training a Classifier - Random Forest\n",
        "\n",
        "The main purpose of this section is to run zero-shot and few-shot experiments using classifiers such as Random Forest, SVM, KNN, Naive Bayes."
      ],
      "metadata": {
        "id": "6vNTirt7spnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing the features"
      ],
      "metadata": {
        "id": "uD5rh7iIv5Iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "gWJe8GLzsnim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "CwrPUeeZv7xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and testing the model"
      ],
      "metadata": {
        "id": "ivqdSAhgwBp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")"
      ],
      "metadata": {
        "id": "l5UJnhXSssL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "YityesL1sumj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = rf.predict(X_test_scaled)\n",
        "print(\"Zero-shot accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Ly1BmB4mswNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting Class Confusions"
      ],
      "metadata": {
        "id": "8-896XqUuE1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot(\n",
        "    xticks_rotation='vertical', cmap='Blues'\n",
        ")\n",
        "plt.title(\"Zero-shot ESC-50 Confusion Matrix (Random Forest)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c-Qy64qeKtUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 8: Visualizing the embedding space\n",
        "\n",
        "The purpose here is to see the clusters of sound categories that came out of our classifier."
      ],
      "metadata": {
        "id": "n6-uES75suDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 9: Transfer Learning / Fine-Tuning"
      ],
      "metadata": {
        "id": "qhV-AdjNs0iQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 10:"
      ],
      "metadata": {
        "id": "yqcsANaOs7Qz"
      }
    }
  ]
}