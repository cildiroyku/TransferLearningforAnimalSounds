{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOX9nYwly6WwRulA6nE/tBm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cildiroyku/TransferLearningforAnimalSounds/blob/main/DLProjectSetupandTemplateforExperimentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning Notebook\n",
        "\n",
        "Instructions:\n",
        "\n",
        "* This notebook has dependencies from a GitHub repo for the project that includes a config file, the path to YAMNet, embedding transofrmations for PyTorch. **Run Part 1 completely before starting to work on this notebook.**\n",
        "\n",
        "* Part 2 is for demonstration purposes only on one sample audio clip. It was used to investigate the inner workings of YAMNet and understand how the embeddings need to be extracted and structured, to be able to process them. **Part 2 is optional to run.**\n",
        "\n",
        "* Part 3 sets up a pipeline to extract embeddings. **Part 3 it should be run before proceeding.**\n",
        "\n",
        "*"
      ],
      "metadata": {
        "id": "Zbux8G-IjCdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part1: Setting up to clone the repo from GitHub, get dependencies, model weights and connect to GPU\n",
        "\n",
        "No need to modify this section. If GPU is not available, model will run on CPU.\n"
      ],
      "metadata": {
        "id": "BoP9RMgvQkkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cildiroyku/TransferLearningforAnimalSounds.git\n",
        "%cd TransferLearningforAnimalSounds\n",
        "!ls -lh yamnet.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t6QCzRuJM98",
        "outputId": "2dfe1e0c-70b3-428b-ee78-0038812bc820"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TransferLearningforAnimalSounds'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 29 (delta 4), reused 12 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (29/29), 13.33 MiB | 16.92 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n",
            "/content/TransferLearningforAnimalSounds\n",
            "-rw-r--r-- 1 root root 15M Nov 10 14:58 yamnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports and configuration load"
      ],
      "metadata": {
        "id": "x0-bZsbpOvJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "cfg = yaml.safe_load(open(\"configs/base.yaml\"))\n",
        "print(cfg[\"data\"][\"sample_rate\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvExCLz_LXec",
        "outputId": "d848afab-9d04-4afb-faea-d5bd30aa39c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_config.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKnY20oqOY0W",
        "outputId": "fe2f5d10-342b-4e1f-c7ab-277ea7fff21a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config loaded successfully!\n",
            "Project name: transfer_learning_for_animal_sounds\n",
            "Sample rate: 16000\n",
            "Device preference: cuda\n",
            "\n",
            "Verified folder: data/gtzan/\n",
            "Verified folder: data/esc50/\n",
            "Verified folder: outputs/embeddings/\n",
            "\n",
            "GPU not detected ‚Äî using CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/w-hc/torch_audioset.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1J5cZOKSnVh",
        "outputId": "32202c92-96a5-439f-8b2f-8cf8a7de740b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torch_audioset'...\n",
            "remote: Enumerating objects: 269, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/32)\u001b[K\rremote: Counting objects:   6% (2/32)\u001b[K\rremote: Counting objects:   9% (3/32)\u001b[K\rremote: Counting objects:  12% (4/32)\u001b[K\rremote: Counting objects:  15% (5/32)\u001b[K\rremote: Counting objects:  18% (6/32)\u001b[K\rremote: Counting objects:  21% (7/32)\u001b[K\rremote: Counting objects:  25% (8/32)\u001b[K\rremote: Counting objects:  28% (9/32)\u001b[K\rremote: Counting objects:  31% (10/32)\u001b[K\rremote: Counting objects:  34% (11/32)\u001b[K\rremote: Counting objects:  37% (12/32)\u001b[K\rremote: Counting objects:  40% (13/32)\u001b[K\rremote: Counting objects:  43% (14/32)\u001b[K\rremote: Counting objects:  46% (15/32)\u001b[K\rremote: Counting objects:  50% (16/32)\u001b[K\rremote: Counting objects:  53% (17/32)\u001b[K\rremote: Counting objects:  56% (18/32)\u001b[K\rremote: Counting objects:  59% (19/32)\u001b[K\rremote: Counting objects:  62% (20/32)\u001b[K\rremote: Counting objects:  65% (21/32)\u001b[K\rremote: Counting objects:  68% (22/32)\u001b[K\rremote: Counting objects:  71% (23/32)\u001b[K\rremote: Counting objects:  75% (24/32)\u001b[K\rremote: Counting objects:  78% (25/32)\u001b[K\rremote: Counting objects:  81% (26/32)\u001b[K\rremote: Counting objects:  84% (27/32)\u001b[K\rremote: Counting objects:  87% (28/32)\u001b[K\rremote: Counting objects:  90% (29/32)\u001b[K\rremote: Counting objects:  93% (30/32)\u001b[K\rremote: Counting objects:  96% (31/32)\u001b[K\rremote: Counting objects: 100% (32/32)\u001b[K\rremote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 269 (delta 28), reused 26 (delta 26), pack-reused 237 (from 1)\u001b[K\n",
            "Receiving objects: 100% (269/269), 662.51 KiB | 9.60 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls torch_audioset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymD-LJePTHMx",
        "outputId": "4e1faedd-0775-434c-9ddb-5d0fb672bbb1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LICENSE    README.md  test\t  tools\t\t  visualization.py\n",
            "notebooks  setup.py   tf_2_torch  torch_audioset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find torch_audioset -maxdepth 2 -type f -name \"*yamnet*.py\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI9s2Fj7TONz",
        "outputId": "548fae51-0587-449f-888e-b0988282ac57"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch_audioset/tf_2_torch/store_yamnet_pred_metadata_as_yml.py\n",
            "torch_audioset/tf_2_torch/convert_yamnet.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!realpath torch_audioset/tf_2_torch/convert_yamnet.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7krepf8T8cu",
        "outputId": "41555660-a477-43c8-fbb2-d72064ac23d8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TransferLearningforAnimalSounds/torch_audioset/torch_audioset/torch_audioset/tf_2_torch/convert_yamnet.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find torch_audioset -type f -name \"params.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSc8fiYMUpRC",
        "outputId": "362a684a-ac72-4281-aae9-f1927d93e498"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch_audioset/torch_audioset/params.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find torch_audioset -type f -name \"model.py\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9sF8BmKVeFq",
        "outputId": "5d550804-edfa-49f9-c0bc-dd05aecd4afe"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch_audioset/torch_audioset/yamnet/model.py\n",
            "torch_audioset/torch_audioset/vggish/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/TransferLearningforAnimalSounds/torch_audioset\")\n",
        "\n",
        "from torch_audioset.yamnet.model import yamnet as YamNet"
      ],
      "metadata": {
        "id": "rpv0iFKVWYhF"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/TransferLearningforAnimalSounds\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JjXJ26gWrwq",
        "outputId": "c11b0945-79b4-4b9f-ba66-ea1ed45c46e4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TransferLearningforAnimalSounds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls configs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN4AOQ1VW38t",
        "outputId": "d1c52f8e-39be-4521-9cdf-6c183cfc7c2f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, yaml\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "with open(\"configs/base.yaml\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "model = YamNet().to(device)\n",
        "checkpoint = torch.load(\"yamnet.pth\", map_location=device)\n",
        "model.load_state_dict(checkpoint)\n",
        "model.eval()\n",
        "\n",
        "print(\"YAMNet model ready on\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkhe5BTUWbX4",
        "outputId": "e43a606c-8ec4-4934-bd29-1ae963cc5884"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YAMNet model ready on cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2:Testing YAMNet on a sample, real audio clip from the ESC-50 dataset\n",
        "\n",
        "**What this section does**\n",
        "\n",
        "* End-to-end flow from raw audio ‚Üí embedding\n",
        "* Fixing TensorFlow--> Pytorch compatibility for embedding sizes\n",
        "* Creating a custom function to extract yamnet embeddings up to layer14\n",
        "* Embedding matching of the correct feature dimension"
      ],
      "metadata": {
        "id": "N0F3uWAYX5X_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading a test sound to check for sample rate and waveform"
      ],
      "metadata": {
        "id": "WTVM6JkvXNob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import numpy as np\n",
        "\n",
        "# Example environmental sound (dog bark)\n",
        "url = \"https://github.com/karoldvl/ESC-50/raw/master/audio/1-100032-A-0.wav\"\n",
        "waveform, sr = torchaudio.load(url)\n",
        "\n",
        "print(\"Original sample rate:\", sr, \"| waveform shape:\", waveform.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1MGu3uKXMq7",
        "outputId": "5434b88a-8051-41ec-b466-22b25e37f76f"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sample rate: 44100 | waveform shape: torch.Size([1, 220500])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing audio file imported aboce"
      ],
      "metadata": {
        "id": "MEcSlEWVXdsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_sr = cfg[\"data\"][\"sample_rate\"]\n",
        "\n",
        "# Resample if needed\n",
        "if sr != target_sr:\n",
        "    waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
        "\n",
        "# Convert to mono and normalize\n",
        "waveform = waveform.mean(dim=0, keepdim=True)\n",
        "waveform = waveform / waveform.abs().max()\n",
        "\n",
        "print(\"Processed waveform shape:\", waveform.shape)  # [1, N]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_sFgCQIYYVM",
        "outputId": "bcfca035-8e81-4cb7-b5c5-16002ea1fcc4"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed waveform shape: torch.Size([1, 80000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters consistent with YAMNet's frontend\n",
        "mel_transform = T.MelSpectrogram(\n",
        "    sample_rate=target_sr,\n",
        "    n_fft=1024,\n",
        "    hop_length=160,\n",
        "    n_mels=64,\n",
        "    f_min=125,\n",
        "    f_max=7500,\n",
        ")\n",
        "db_transform = T.AmplitudeToDB(stype=\"power\")\n",
        "\n",
        "# Convert\n",
        "mel_spec = mel_transform(waveform)\n",
        "mel_spec_db = db_transform(mel_spec)\n",
        "\n",
        "print(\"Mel-spectrogram shape:\", mel_spec_db.shape)  # [1, 64, T]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltnnN5UKXafw",
        "outputId": "e4170851-7b05-4f2e-e1c2-dc45bd5c7030"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mel-spectrogram shape: torch.Size([1, 64, 501])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YAMNet expects [frames, mel_bins], not batched [1, 64, T]\n",
        "mel_spec_db = mel_spec_db.transpose(1, 2)  # ‚Üí [1, T, 64]\n",
        "mel_spec_db = mel_spec_db.squeeze(0)       # ‚Üí [T, 64]\n",
        "\n",
        "print(\"Final mel input shape for YAMNet:\", mel_spec_db.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS6MwyHDa8MI",
        "outputId": "7cccf83b-39a7-41fb-a85c-459f8c7288ed"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final mel input shape for YAMNet: torch.Size([501, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mel_spec_db = mel_spec_db.unsqueeze(0).unsqueeze(0)\n",
        "print(\"Final mel input shape for YAMNet:\", mel_spec_db.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZPkW-YtcEXg",
        "outputId": "3080a94d-26e5-4300-9c74-e69b3442d86f"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final mel input shape for YAMNet: torch.Size([1, 1, 501, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Jq7ApvJsdCXm",
        "outputId": "2cfbf6fa-0904-409d-d4d8-08986bdb3207"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YAMNet(\n",
            "  (layer1): Conv(\n",
            "    (fused): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer2): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=32, bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer3): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=SAME, groups=64, bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(64, 128, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer4): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=128, bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer5): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=SAME, groups=128, bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(128, 256, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer6): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=256, bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(256, 256, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer7): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=SAME, groups=256, bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(256, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer8): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=512, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer9): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=512, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer10): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=512, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer11): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=512, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer12): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=512, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer13): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=SAME, groups=512, bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(512, 1024, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(1024, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer14): SeparableConv(\n",
            "    (depthwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=SAME, groups=1024, bias=False)\n",
            "      (bn): BatchNorm2d(1024, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (pointwise_conv): CONV_BN_RELU(\n",
            "      (conv): Conv2d_tf(1024, 1024, kernel_size=(1, 1), stride=(1, 1), padding=SAME, bias=False)\n",
            "      (bn): BatchNorm2d(1024, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=1024, out_features=521, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_yamnet_embedding(model, mel_features):\n",
        "    \"\"\"\n",
        "    Runs YAMNet backbone up to layer14 ‚Üí returns 1024-D embedding\n",
        "    Input shape: [1, 1, frames, 64]\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        x = model.layer1(mel_features)\n",
        "        x = model.layer2(x)\n",
        "        x = model.layer3(x)\n",
        "        x = model.layer4(x)\n",
        "        x = model.layer5(x)\n",
        "        x = model.layer6(x)\n",
        "        x = model.layer7(x)\n",
        "        x = model.layer8(x)\n",
        "        x = model.layer9(x)\n",
        "        x = model.layer10(x)\n",
        "        x = model.layer11(x)\n",
        "        x = model.layer12(x)\n",
        "        x = model.layer13(x)\n",
        "        x = model.layer14(x)\n",
        "\n",
        "        # Global average pooling across spatial dims\n",
        "        x = torch.mean(x, dim=[2, 3])  # [batch, 1024]\n",
        "        return x.squeeze(0)\n"
      ],
      "metadata": {
        "id": "5u_Dang6ZN0J"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passing through YAMNet to extract embeddings"
      ],
      "metadata": {
        "id": "KfBR5EdXXnTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_embedding = extract_yamnet_embedding(model, mel_spec_db)\n",
        "print(\"Embedding shape:\", clip_embedding.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-0BjH1Cdb9V",
        "outputId": "0e9e92fc-c1e7-4f51-d2e7-e02291cd6b6e"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: torch.Size([1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Making a reusable pipeline to extract YAMNet embeddings"
      ],
      "metadata": {
        "id": "RDvlkcQ1eLay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torch\n",
        "\n",
        "def yamnet_extract_from_file(model, file_path, cfg):\n",
        "    \"\"\"\n",
        "    Full pipeline: loads audio, converts to log-mel, extracts 1024-D embedding.\n",
        "    Returns a PyTorch tensor [1024].\n",
        "    \"\"\"\n",
        "    target_sr = cfg[\"data\"][\"sample_rate\"]\n",
        "\n",
        "    # --- Load and resample ---\n",
        "    waveform, sr = torchaudio.load(file_path)\n",
        "    if sr != target_sr:\n",
        "        waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
        "    waveform = waveform.mean(dim=0, keepdim=True)\n",
        "    waveform = waveform / waveform.abs().max()\n",
        "\n",
        "    # --- Mel spectrogram ---\n",
        "    mel_transform = T.MelSpectrogram(\n",
        "        sample_rate=target_sr,\n",
        "        n_fft=1024,\n",
        "        hop_length=160,\n",
        "        n_mels=64,\n",
        "        f_min=125,\n",
        "        f_max=7500,\n",
        "    )\n",
        "    db_transform = T.AmplitudeToDB(stype=\"power\")\n",
        "    mel_spec = mel_transform(waveform)\n",
        "    mel_spec_db = db_transform(mel_spec)\n",
        "\n",
        "    # --- Prepare for YAMNet ---\n",
        "    mel_spec_db = mel_spec_db.transpose(1, 2).unsqueeze(0)\n",
        "\n",
        "\n",
        "    # --- Forward through YAMNet backbone ---\n",
        "    with torch.no_grad():\n",
        "        x = model.layer1(mel_spec_db)\n",
        "        x = model.layer2(x)\n",
        "        x = model.layer3(x)\n",
        "        x = model.layer4(x)\n",
        "        x = model.layer5(x)\n",
        "        x = model.layer6(x)\n",
        "        x = model.layer7(x)\n",
        "        x = model.layer8(x)\n",
        "        x = model.layer9(x)\n",
        "        x = model.layer10(x)\n",
        "        x = model.layer11(x)\n",
        "        x = model.layer12(x)\n",
        "        x = model.layer13(x)\n",
        "        x = model.layer14(x)\n",
        "        x = torch.mean(x, dim=[2, 3])  # [batch, 1024]\n",
        "\n",
        "    return x.squeeze(0)"
      ],
      "metadata": {
        "id": "GaULiDpQeOve"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/karoldvl/ESC-50/raw/master/audio/1-100032-A-0.wav\"\n",
        "waveform, sr = torchaudio.load(url)"
      ],
      "metadata": {
        "id": "XuZr5-z3iU74"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample usage:\n",
        "\n",
        "path = \"/content/TransferLearningforAnimalSounds/data/esc50/1-100032-A-0.wav\"\n",
        "torch.save(waveform, \"sample_waveform.pt\")\n",
        "emb = yamnet_extract_from_file(model, url, cfg)\n",
        "\n",
        "print(emb.shape)  # torch.Size([1024])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5XpejrTeSZw",
        "outputId": "b2b4ec34-38bd-4fee-94f8-703454f5d62d"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Loading the Data for Extraction\n",
        "\n",
        "Make sure you point the extraction function below to where you have the audio files from ESC-50 saved on your Google Drive."
      ],
      "metadata": {
        "id": "Js_A7cNEkvAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK4p3z_Npx3R",
        "outputId": "3c636d45-051e-41c4-d5ef-a6b27862ac9c"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/DeepLearning/ESC-50-master/audio\" /content/audio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1buJMzHqBUk",
        "outputId": "000f7ce3-bbf7-45a0-ad1e-dcaf2dc0febf"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/DeepLearning/ESC-50-master/audio': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Creating a Pipeline That Processes All Audio Clips in Batch\n",
        "\n",
        "Building a function that:\n",
        "\n",
        "* walks through a dataset folder (e.g. data/esc50),\n",
        "\n",
        "* extracts one 1024-D embedding per audio file,\n",
        "\n",
        "* saves each embedding to a structured directory (outputs/embeddings/{dataset_name}/),\n",
        "\n",
        "* writes a summary .csv or .npy file for downstream training."
      ],
      "metadata": {
        "id": "7QOXXw-becjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "def extract_embeddings_from_folder(model, folder_path, cfg, save_dir):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    embeddings, labels = [], []\n",
        "\n",
        "    # Glob reference folder_path\n",
        "    wav_files = glob.glob(os.path.join(folder_path, \"*.wav\"))\n",
        "    print(f\"üîç Found {len(wav_files)} WAV files in: {folder_path}\")\n",
        "\n",
        "    if len(wav_files) == 0:\n",
        "        print(\"‚ö†Ô∏è No .wav files found ‚Äî check your folder path!\")\n",
        "        return\n",
        "\n",
        "    for file_path in tqdm(wav_files):\n",
        "        emb = yamnet_extract_from_file(model, file_path, cfg)\n",
        "        embeddings.append(emb.numpy())\n",
        "        labels.append(os.path.basename(file_path).split(\"-\")[0])  # optional: parse label\n",
        "\n",
        "    embeddings = np.stack(embeddings)\n",
        "    np.save(os.path.join(save_dir, \"embeddings.npy\"), embeddings)\n",
        "    np.save(os.path.join(save_dir, \"labels.npy\"), np.array(labels))\n",
        "    print(f\"‚úÖ Saved embeddings: {embeddings.shape} | Labels: {len(labels)}\")"
      ],
      "metadata": {
        "id": "VWmHl0hwegUx"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_embeddings_from_folder(\n",
        "    model,\n",
        "    \"/content/drive/MyDrive/DeepLearning/ESC-50-master/audio\",\n",
        "    cfg,\n",
        "    \"outputs/embeddings/esc50\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4DS_6Qyk7Qg",
        "outputId": "d41b8b27-5706-47ff-f222-a4ecc381c645"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Found 2000 WAV files in: /content/drive/MyDrive/DeepLearning/ESC-50-master/audio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [01:56<00:00, 17.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved embeddings: (2000, 1024) | Labels: 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Loading embeddings as predictors and labels\n",
        "\n"
      ],
      "metadata": {
        "id": "gF60jDgvse5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.load(\"outputs/embeddings/esc50/embeddings.npy\")\n",
        "y = np.load(\"outputs/embeddings/esc50/labels.npy\")\n",
        "\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlP2QU6Os_cv",
        "outputId": "596f33b6-2a15-44ad-9c85-b198ca065f56"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2000, 1024) (2000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "0E05RlHGvTiE"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7: Training a Classifier - Log Regression"
      ],
      "metadata": {
        "id": "6vNTirt7spnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing the features"
      ],
      "metadata": {
        "id": "uD5rh7iIv5Iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "CwrPUeeZv7xX"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and testing the model"
      ],
      "metadata": {
        "id": "ivqdSAhgwBp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 8: Visualizing the embedding space\n",
        "\n",
        "The purpose here is to see the clusters of sound categories that came out of our classifier."
      ],
      "metadata": {
        "id": "n6-uES75suDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 9: Transfer Learning / Fine-Tuning"
      ],
      "metadata": {
        "id": "qhV-AdjNs0iQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 10:"
      ],
      "metadata": {
        "id": "yqcsANaOs7Qz"
      }
    }
  ]
}